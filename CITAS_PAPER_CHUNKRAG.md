# Citas del Paper ChunkRAG por TÃ©cnica

**Paper**: ChunkRAG: A Novel LLM-Chunk Filtering Method for RAG Systems
**arXiv**: 2410.19572v5
**Fecha**: 23 Apr 2025

---

## Ãndice de TÃ©cnicas

| # | TÃ©cnica | Estado | PÃ¡gina Principal |
|---|---------|--------|------------------|
| 1 | [Semantic Chunking](#1-semantic-chunking) | âœ… Implementada | PÃ¡gina 2-3 |
| 2 | [Multi-Stage Relevance Scoring](#2-multi-stage-relevance-scoring) | âœ… Implementada | PÃ¡gina 3-4 |
| 3 | [Dynamic Thresholding](#3-dynamic-thresholding) | âœ… Implementada | PÃ¡gina 4, Algorithm 1 |
| 4 | [Chunk-Level Filtering](#4-chunk-level-filtering) | âœ… Implementada | PÃ¡gina 1-2 |
| 5 | [Redundancy Removal](#5-redundancy-removal) | âŒ No implementada | PÃ¡gina 2, 4 |
| 6 | [Hybrid Retrieval (BM25 + LLM)](#6-hybrid-retrieval-bm25--llm) | âŒ No implementada | PÃ¡gina 4 |
| 7 | [Cohere Reranking](#7-cohere-reranking) | âŒ No implementada | PÃ¡gina 4 |

---

## TÃ‰CNICAS IMPLEMENTADAS âœ…

---

## 1. Semantic Chunking

### ğŸ“ UbicaciÃ³n en el Paper

#### SecciÃ³n Principal
- **PÃ¡gina**: 2-3
- **SecciÃ³n**: "3 Methodology â†’ Semantic Chunking"
- **LÃ­neas**: Primera subsecciÃ³n de Methodology

### ğŸ”– Citas Textuales a Resaltar

#### Cita 1: DefiniciÃ³n del Proceso
**PÃ¡gina 2, SecciÃ³n 3**
```
"Semantic chunking serves as the foundational step of our methodology,
transforming the input document into semantically meaningful units to
facilitate effective retrieval."
```

#### Cita 2: Los 3 Subprocesos
**PÃ¡gina 3, SecciÃ³n "Semantic Chunking"**
```
"This stage involves three sub-processes:

â€¢ Input Preparation: We begin by tokenizing a document D into sentences
  using NLTK's sent_tokenize function. Each sentence is then assigned an
  embedding vector, generated using a pre-trained embedding model
  (text-embedding-3-small).

â€¢ Chunk Formation: Consecutive sentences are grouped into chunks based on
  their semantic similarity, measured by cosine similarity. Specifically,
  if the similarity between consecutive sentences drops below a threshold
  (Î¸ = 0.8), a new chunk is created, as this indicates a shift to a
  different subtopic or theme that warrants its own grouping.

â€¢ Chunk Embeddings: Each chunk is represented using the same pre-trained
  embedding model as above. The resultant chunk embeddings are stored in
  a vector database to facilitate efficient retrieval during the query phase."
```

#### Cita 3: ParÃ¡metros EspecÃ­ficos
**PÃ¡gina 3**
```
"if the similarity between consecutive sentences drops below a threshold
(Î¸ = 0.8), a new chunk is created"

"Each chunk is also further constrained to be under 500 characters to
enable granular search and prevent oversized chunks"
```

### ğŸ“Š Referencias Adicionales

- **Figura**: Figure 2 (pÃ¡gina 3) - Muestra "Semantic Chunking" en el pipeline
- **Tabla**: Table 2 (pÃ¡gina 7) - "Chunk Analysis Across Similarity Thresholds"

### âœï¸ QuÃ© Resaltar

```
PÃGINA 2-3, SECCIÃ“N "3 METHODOLOGY - Semantic Chunking":
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ… PÃ¡rrafo completo de "Semantic chunking serves..."   â”‚
â”‚ âœ… Los 3 bullets (Input Preparation, Chunk Formation,   â”‚
â”‚    Chunk Embeddings)                                    â”‚
â”‚ âœ… "Î¸ = 0.8" (threshold)                                â”‚
â”‚ âœ… "under 500 characters"                               â”‚
â”‚ âœ… "text-embedding-3-small"                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. Multi-Stage Relevance Scoring

### ğŸ“ UbicaciÃ³n en el Paper

#### SecciÃ³n Principal
- **PÃ¡gina**: 3-4
- **SecciÃ³n**: "3 Methodology â†’ Hybrid Retrieval and Advanced Filtering â†’ Relevance Scoring and Tresholding"
- **Sub-tÃ­tulo**: "Relevance Scoring and Tresholding" (nota: typo en el paper, dice "Tresholding" en vez de "Thresholding")

### ğŸ”– Citas Textuales a Resaltar

#### Cita 1: DescripciÃ³n del Proceso Multi-Stage
**PÃ¡gina 4, SecciÃ³n "Relevance Scoring and Tresholding"**
```
"Each chunk's relevance is evaluated through a multi-stage process:
an LLM assigns initial scores, followed by self-reflection and critic
model refinements. The self-reflection step assesses query alignment,
while the critic applies domain-specific heuristics (e.g., temporal
consistency for time-sensitive queries)."
```

#### Cita 2: LÃ­neas del Algorithm 1
**PÃ¡gina 5, Algorithm 1, lÃ­neas 11-17**
```
11: // Multi-stage Scoring
12: for each chunk c âˆˆ Cfiltered do
13:     base â† LLMRelevance(c, qrewritten)
14:     reflect â† SelfReflect(c, qrewritten, base)
15:     critic â† CriticEval(c, qrewritten, base, reflect)
16:     score(c) â† CombineScores(base, reflect, critic)
17: end for
```

#### Cita 3: Self-Reflection Prompt (Appendix)
**PÃ¡gina 10, Appendix A.1, "Self-Reflection Prompt"**
```
"You have assigned a relevance score to a text chunk based on a user query.
Your initial score was: {score}

Reflect on your scoring and adjust the score if necessary.
Provide the final score."
```

### ğŸ“Š Referencias Adicionales

- **Algorithm 1**: LÃ­neas 11-17 (pÃ¡gina 5)
- **Figure 2**: "LLM-Based Scoring" box muestra "Initial Score â†’ Self-Reflection â†’ Critic LLM Scoring"
- **Appendix A.1**: PÃ¡gina 10 - Todos los prompts (Relevance Scoring, Self-Reflection)

### âœï¸ QuÃ© Resaltar

```
PÃGINA 4, SECCIÃ“N "Relevance Scoring and Tresholding":
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ… "multi-stage process: an LLM assigns initial scores, â”‚
â”‚    followed by self-reflection and critic model         â”‚
â”‚    refinements"                                         â”‚
â”‚ âœ… "self-reflection step assesses query alignment"      â”‚
â”‚ âœ… "critic applies domain-specific heuristics"          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PÃGINA 5, ALGORITHM 1, LÃNEAS 11-17:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ… Resaltar todo el bloque "// Multi-stage Scoring"    â”‚
â”‚ âœ… Las 3 funciones: LLMRelevance, SelfReflect,          â”‚
â”‚    CriticEval                                           â”‚
â”‚ âœ… CombineScores (combina los 3 scores)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PÃGINA 10, APPENDIX A.1:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ… "Relevance Scoring Prompt" completo                  â”‚
â”‚ âœ… "Self-Reflection Prompt" completo                    â”‚
â”‚ âœ… (Opcional) "Critic" no tiene prompt separado en      â”‚
â”‚    appendix, pero se menciona en lÃ­nea 15 del algoritmoâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. Dynamic Thresholding

### ğŸ“ UbicaciÃ³n en el Paper

#### SecciÃ³n Principal
- **PÃ¡gina**: 4 (descripciÃ³n), 5 (algoritmo)
- **SecciÃ³n**: "3 Methodology â†’ Relevance Scoring and Tresholding"
- **Algorithm 1**: LÃ­neas 18-22

### ğŸ”– Citas Textuales a Resaltar

#### Cita 1: DescripciÃ³n del Dynamic Threshold
**PÃ¡gina 4, SecciÃ³n "Relevance Scoring and Tresholding"**
```
"A dynamic threshold, based on score distribution analysis, determines
final chunk selection. When scores cluster tightly, the threshold
increases to retain only the most relevant chunks."
```

#### Cita 2: Algorithm 1 - Dynamic Thresholding
**PÃ¡gina 5, Algorithm 1, lÃ­neas 18-22**
```
18: // Dynamic Thresholding
19: S â† { score(c) | c âˆˆ Cfiltered}
20: Î¼ â† mean(S); Ïƒ â† std(S)
21: T â† if var(S) < Ïµ then Î¼ + Ïƒ else Î¼
22: Cthreshold â† { c âˆˆ Cfiltered | score(c) â‰¥ T}
```

**ESTA ES LA FÃ“RMULA CLAVE** â­

#### Cita 3: ExplicaciÃ³n en DiscusiÃ³n
**PÃ¡gina 7, SecciÃ³n "7 Discussion"**
```
"The ablation study highlights redundancy filtering's key role in
ChunkRAG, with dynamic chunk merging and optimal similarity thresholds
(validated at Î¸ = 0.8) balancing chunk reduction and relevance while
preventing over-filtering."
```

### ğŸ“Š Referencias Adicionales

- **Algorithm 1**: LÃ­nea 21 contiene la fÃ³rmula del threshold
- **Figure 3**: PÃ¡gina 6 - "Chunk Reduction vs. Similarity Threshold"
- **Table 2**: PÃ¡gina 7 - Muestra valores de threshold 0.5-0.9

### âœï¸ QuÃ© Resaltar

```
PÃGINA 4, SECCIÃ“N "Relevance Scoring and Tresholding":
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ… "A dynamic threshold, based on score distribution    â”‚
â”‚    analysis, determines final chunk selection"          â”‚
â”‚ âœ… "When scores cluster tightly, the threshold increasesâ”‚
â”‚    to retain only the most relevant chunks"             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PÃGINA 5, ALGORITHM 1, LÃNEAS 18-22:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â­â­â­ RESALTAR CON FLUORESCENTE â­â­â­                   â”‚
â”‚                                                         â”‚
â”‚ âœ… LÃ­nea 21: T â† if var(S) < Ïµ then Î¼ + Ïƒ else Î¼       â”‚
â”‚                                                         â”‚
â”‚ Esta es la fÃ³rmula matemÃ¡tica del threshold dinÃ¡mico   â”‚
â”‚ ExplicaciÃ³n:                                            â”‚
â”‚   - Si varianza baja: threshold = media + desv_std     â”‚
â”‚   - Si varianza alta: threshold = media                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PÃGINA 7, TABLE 2:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ… Toda la tabla "Chunk Analysis Across Similarity      â”‚
â”‚    Thresholds"                                          â”‚
â”‚ âœ… Especialmente Î¸ = 0.8 (el Ã³ptimo validado)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4. Chunk-Level Filtering

### ğŸ“ UbicaciÃ³n en el Paper

#### SecciÃ³n Principal
- **PÃ¡gina**: 1-2 (introducciÃ³n y problema), 3-4 (metodologÃ­a)
- **SecciÃ³n**: "1 Introduction" y "3 Methodology"

### ğŸ”– Citas Textuales a Resaltar

#### Cita 1: DefiniciÃ³n del Problema (IntroducciÃ³n)
**PÃ¡gina 1-2, SecciÃ³n "1 Introduction"**
```
"Current RAG systems often retrieve large document segments, assuming
more content means better coverage. However, this overlooks the need
to evaluate smaller sections independently, leading to the inclusion
of irrelevant information."
```

#### Cita 2: Propuesta de ChunkRAG
**PÃ¡gina 2, SecciÃ³n "1 Introduction"**
```
"We propose ChunkRAG, a novel approach of LLM-driven chunk filtering.
This framework operates at a finer level of granularity than traditional
systems by supporting chunk-level filtering of retrieved information.
Rather than determining the relevance of entire documents, our framework
evaluates both the user query and the individual chunks within the
retrieved chunks."
```

#### Cita 3: Beneficio del Chunk-Level Filtering
**PÃ¡gina 2, Abstract**
```
"The analysis further demonstrates that chunk-level filtering reduces
redundant and weakly related information, enhancing the factual
consistency of responses."
```

#### Cita 4: ComparaciÃ³n con Document-Level
**PÃ¡gina 1, Abstract**
```
"Existing document-level retrieval approaches lack sufficient granularity
to effectively filter non-essential content."
```

### ğŸ“Š Referencias Adicionales

- **Figure 1**: PÃ¡gina 1 - ComparaciÃ³n visual "With and Without Chunk Filtering"
- **Section 5.2**: PÃ¡gina 6 - "Insights" explica por quÃ© chunk-level es mejor

### âœï¸ QuÃ© Resaltar

```
PÃGINA 1, ABSTRACT:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ… "Existing document-level retrieval approaches lack   â”‚
â”‚    sufficient granularity to effectively filter         â”‚
â”‚    non-essential content"                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PÃGINA 1-2, INTRODUCTION:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ… "Current RAG systems often retrieve large document   â”‚
â”‚    segments, assuming more content means better coverageâ”‚
â”‚ âœ… "This framework operates at a finer level of         â”‚
â”‚    granularity than traditional systems by supporting   â”‚
â”‚    chunk-level filtering"                               â”‚
â”‚ âœ… "Rather than determining the relevance of entire     â”‚
â”‚    documents, our framework evaluates... individual     â”‚
â”‚    chunks"                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PÃGINA 6, SECTION 5.2 "Insights":
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ… "chunk-level filtering offers greater benefits in    â”‚
â”‚    short, fact-intensive tasks like PopQAâ€”where even    â”‚
â”‚    minor irrelevant segments can lead to hallucinations"â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

FIGURA 1 (PÃGINA 1):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ… Toda la figura mostrando:                            â”‚
â”‚    - Sin filtering: respuesta con info irrelevante      â”‚
â”‚    - Con LLM chunk filtering: respuesta precisa         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## TÃ‰CNICAS NO IMPLEMENTADAS âŒ

---

## 5. Redundancy Removal

### ğŸ“ UbicaciÃ³n en el Paper

#### SecciÃ³n Principal
- **PÃ¡gina**: 2 (Related Works), 4 (Methodology)
- **SecciÃ³n**: "2.4 Redundancy Reduction with Cosine Similarity" y "Initial Filtering"

### ğŸ”– Citas Textuales a Resaltar

#### Cita 1: DescripciÃ³n en Related Works
**PÃ¡gina 2, SecciÃ³n "2.4 Redundancy Reduction with Cosine Similarity"**
```
"Redundant information in retrieved documents can clutter context.
Using cosine similarity, near-identical sections can be deduplicated
by filtering chunks exceeding a similarity threshold (e.g., > 0.9)
(Gan et al., 2024), streamlining input and reducing confusion from
repetition."
```

**THRESHOLD CLAVE**: > 0.9 â­

#### Cita 2: ImplementaciÃ³n en Methodology
**PÃ¡gina 4, SecciÃ³n "Initial Filtering"**
```
"Retrieved chunks are initially filtered using a combination of TF-IDF
scoring and cosine similarity. Chunks with high redundancy
(similarity > 0.9) are eliminated."
```

#### Cita 3: En Algorithm 1
**PÃ¡gina 5, Algorithm 1, lÃ­neas 4-10**
```
4: // Redundancy Removal
5: Cfiltered â† âˆ…
6: for each chunk ci âˆˆ C do
7:     if max cos(emb(ci), emb(cj)) â‰¤ Î»dup then
        cjâˆˆCfiltered
8:         Append ci to Cfiltered
9:     end if
10: end for
```

**ParÃ¡metro**: Î»dup = 0.9 (lÃ­nea 3: `Require: Î»dup: Redundancy threshold (e.g., 0.9)`)

### ğŸ“Š Referencias Adicionales

- **Algorithm 1**: LÃ­neas 4-10 (pÃ¡gina 5)
- **Section 6.1**: PÃ¡gina 6 - "Redundancy Filtering Effectiveness"
- **Figure 3**: PÃ¡gina 6 - Muestra reducciÃ³n de chunks por threshold

### âœï¸ QuÃ© Resaltar

```
PÃGINA 2, SECCIÃ“N "2.4 Redundancy Reduction with Cosine Similarity":
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ… Toda la secciÃ³n completa (4-5 lÃ­neas)                â”‚
â”‚ â­ "similarity threshold (e.g., > 0.9)"                 â”‚
â”‚ âœ… "deduplicated by filtering chunks exceeding"         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PÃGINA 4, SECCIÃ“N "Initial Filtering":
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ… "Chunks with high redundancy (similarity > 0.9) are  â”‚
â”‚    eliminated"                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PÃGINA 5, ALGORITHM 1, LÃNEAS 4-10:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â­â­ RESALTAR TODO EL BLOQUE â­â­                        â”‚
â”‚                                                         â”‚
â”‚ âœ… LÃ­nea 3: "Î»dup: Redundancy threshold (e.g., 0.9)"   â”‚
â”‚ âœ… LÃ­neas 4-10: Algoritmo completo de redundancy removalâ”‚
â”‚ âœ… LÃ­nea 7: CondiciÃ³n "if max cos(...) â‰¤ Î»dup"         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PÃGINA 6, FIGURE 3:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ… GrÃ¡fica "Chunk Reduction vs Similarity Threshold"    â”‚
â”‚ âœ… Nota que en threshold 0.9 â†’ 8.5% reduction          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 6. Hybrid Retrieval (BM25 + LLM)

### ğŸ“ UbicaciÃ³n en el Paper

#### SecciÃ³n Principal
- **PÃ¡gina**: 4
- **SecciÃ³n**: "3 Methodology â†’ Hybrid Retrieval and Advanced Filtering â†’ Hybrid Retrieval Strategy"

### ğŸ”– Citas Textuales a Resaltar

#### Cita 1: DescripciÃ³n del Hybrid Retrieval
**PÃ¡gina 4, SecciÃ³n "Hybrid Retrieval Strategy"**
```
"We combine BM25 and LLM-based retrieval methods with equal weights
(0.5 each) to balance keyword and semantic matching."
```

**PESOS CLAVE**: 0.5 BM25 + 0.5 LLM â­

#### Cita 2: En Algorithm 1
**PÃ¡gina 5, Algorithm 1, lÃ­neas 2-3**
```
2: // Hybrid Retrieval
3: C â† CombineRetrieval(BM25(D, qrewritten), LLM(D, qrewritten), wbm25, wllm)
```

#### Cita 3: ParÃ¡metros del Algorithm 1
**PÃ¡gina 5, Algorithm 1, Requirements**
```
Require: wbm25, wllm: Hybrid retrieval weights
```

### ğŸ“Š Referencias Adicionales

- **Algorithm 1**: LÃ­neas 2-3 (pÃ¡gina 5)
- **Figure 2**: PÃ¡gina 3 - Muestra "Base Retriever" y "BM25 Retriever" con pesos 0.5

### âœï¸ QuÃ© Resaltar

```
PÃGINA 4, SECCIÃ“N "Hybrid Retrieval Strategy":
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â­â­ RESALTAR TODA LA SECCIÃ“N â­â­                       â”‚
â”‚                                                         â”‚
â”‚ âœ… "We combine BM25 and LLM-based retrieval methods     â”‚
â”‚    with equal weights (0.5 each)"                      â”‚
â”‚ âœ… "to balance keyword and semantic matching"           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PÃGINA 5, ALGORITHM 1, LÃNEAS 2-3:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ… LÃ­nea 2: "// Hybrid Retrieval"                      â”‚
â”‚ âœ… LÃ­nea 3: "CombineRetrieval(BM25(...), LLM(...),     â”‚
â”‚             wbm25, wllm)"                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PÃGINA 3, FIGURE 2:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ… "Base Retriever" box                                 â”‚
â”‚ âœ… "BM25 Retriever" box                                 â”‚
â”‚ âœ… "Weight 0.5" labels en ambos                         â”‚
â”‚ âœ… "Ensemble Retriever" que combina ambos               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 7. Cohere Reranking

### ğŸ“ UbicaciÃ³n en el Paper

#### SecciÃ³n Principal
- **PÃ¡gina**: 4
- **SecciÃ³n**: "3 Methodology â†’ Hybrid Retrieval and Advanced Filtering â†’ Hybrid Retrieval Strategy"

### ğŸ”– Citas Textuales a Resaltar

#### Cita 1: DescripciÃ³n del Reranking
**PÃ¡gina 4, SecciÃ³n "Hybrid Retrieval Strategy"**
```
"Cohere's reranking model (rerank-english-v3.0) then addresses the
Lost in the middle problem - where relevant information in the middle
of long documents tends to be underemphasized by standard retrieval
methods - by re-evaluating chunks with emphasis on contextual centrality,
preventing the oversight of relevant mid-document information."
```

**MODELO CLAVE**: rerank-english-v3.0 â­

#### Cita 2: En Algorithm 1
**PÃ¡gina 5, Algorithm 1, lÃ­neas 23-24**
```
23: // Lost-in-Middle Reranking
24: Cfinal â† Cohere_Rerank(Cthreshold, qrewritten)
```

### ğŸ“Š Referencias Adicionales

- **Algorithm 1**: LÃ­neas 23-24 (pÃ¡gina 5) - Ãºltimo paso antes del return
- **Figure 2**: PÃ¡gina 3 - Muestra "COHERE RE-RANK" como paso final

### âœï¸ QuÃ© Resaltar

```
PÃGINA 4, SECCIÃ“N "Hybrid Retrieval Strategy":
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â­â­ TODO EL PÃRRAFO DE COHERE â­â­                      â”‚
â”‚                                                         â”‚
â”‚ âœ… "Cohere's reranking model (rerank-english-v3.0)"    â”‚
â”‚ âœ… "addresses the Lost in the middle problem"           â”‚
â”‚ âœ… "re-evaluating chunks with emphasis on contextual    â”‚
â”‚    centrality"                                          â”‚
â”‚ âœ… "preventing the oversight of relevant mid-document   â”‚
â”‚    information"                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PÃGINA 5, ALGORITHM 1, LÃNEAS 23-24:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ… LÃ­nea 23: "// Lost-in-Middle Reranking"             â”‚
â”‚ âœ… LÃ­nea 24: "Cfinal â† Cohere_Rerank(Cthreshold,       â”‚
â”‚              qrewritten)"                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PÃGINA 3, FIGURE 2:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ… Box final "COHERE RE-RANK"                           â”‚
â”‚ âœ… Es el Ãºltimo paso antes de la respuesta final        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Resumen de PÃ¡ginas Clave

### ğŸ“š Tabla RÃ¡pida de Referencia

| PÃ¡gina | Contenido Clave |
|--------|-----------------|
| **1** | Abstract, IntroducciÃ³n, Figure 1 (chunk filtering comparison) |
| **2** | Related Works, Redundancy (secciÃ³n 2.4), Inicio Methodology |
| **3** | Semantic Chunking (detallado), Figure 2 (pipeline completo) |
| **4** | Multi-stage scoring, Dynamic threshold, Hybrid retrieval, Cohere |
| **5** | **Algorithm 1** (â­ MÃS IMPORTANTE) - Todo el pipeline |
| **6** | Analysis, Figure 3 (redundancy), Figure 4 (similarity) |
| **7** | Table 1 (resultados), Table 2 (thresholds), Discussion |
| **10** | Appendix A.1 - Prompts exactos para LLM scoring |

---

## Mapa Visual del Paper

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PÃGINA 1: ABSTRACT + INTRODUCTION                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… Chunk-level filtering (definiciÃ³n del problema)              â”‚
â”‚ âœ… Figure 1 (comparaciÃ³n visual)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PÃGINA 2: RELATED WORKS                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âŒ SecciÃ³n 2.4: Redundancy Removal                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PÃGINA 3: METHODOLOGY - SEMANTIC CHUNKING                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… Semantic Chunking (3 subprocesos)                            â”‚
â”‚ âœ… Î¸ = 0.8, 500 chars, text-embedding-3-small                  â”‚
â”‚ âœ… Figure 2: Pipeline completo                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PÃGINA 4: METHODOLOGY - FILTERING & RETRIEVAL                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… Multi-stage scoring (base, reflect, critic)                  â”‚
â”‚ âœ… Dynamic thresholding (descripciÃ³n)                           â”‚
â”‚ âŒ Redundancy removal (similarity > 0.9)                        â”‚
â”‚ âŒ Hybrid Retrieval (BM25 0.5 + LLM 0.5)                        â”‚
â”‚ âŒ Cohere rerank-english-v3.0                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PÃGINA 5: ALGORITHM 1 â­â­â­ MÃS IMPORTANTE â­â­â­             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LÃ­neas 1-3:   Hybrid Retrieval (BM25 + LLM)                    â”‚
â”‚ LÃ­neas 4-10:  âŒ Redundancy Removal                             â”‚
â”‚ LÃ­neas 11-17: âœ… Multi-stage Scoring                            â”‚
â”‚ LÃ­neas 18-22: âœ… Dynamic Thresholding (fÃ³rmula Î¼+Ïƒ vs Î¼)       â”‚
â”‚ LÃ­neas 23-24: âŒ Cohere Reranking                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PÃGINA 6-7: RESULTS & ABLATION                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Figure 3: Redundancy effectiveness                              â”‚
â”‚ Table 1: Accuracy results (PopQA 64.9%, PubHealth 77.3%)       â”‚
â”‚ Table 2: Threshold analysis (0.5 to 0.9)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PÃGINA 10: APPENDIX A.1 - PROMPTS                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… Relevance Scoring Prompt (base score)                        â”‚
â”‚ âœ… Self-Reflection Prompt                                       â”‚
â”‚ âœ… Threshold Determination Prompt                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## GuÃ­a de Colores para Resaltado

Si vas a imprimir o marcar el paper fÃ­sicamente:

```
ğŸŸ¡ AMARILLO (Implementadas âœ…):
   - Semantic chunking (pÃ¡ginas 2-3)
   - Multi-stage scoring (pÃ¡gina 4, Algorithm lÃ­neas 11-17)
   - Dynamic thresholding (pÃ¡gina 4, Algorithm lÃ­nea 21)
   - Chunk-level filtering (pÃ¡ginas 1-2)

ğŸŸ¢ VERDE (No implementadas - Alta prioridad âŒ):
   - Redundancy removal (pÃ¡gina 2, 4, Algorithm lÃ­neas 4-10)
   - Hybrid retrieval (pÃ¡gina 4, Algorithm lÃ­neas 2-3)

ğŸ”µ AZUL (No implementadas - Media prioridad âŒ):
   - Cohere reranking (pÃ¡gina 4, Algorithm lÃ­neas 23-24)

ğŸ”´ ROJO (FÃ³rmulas y parÃ¡metros clave â­):
   - Î¸ = 0.8 (semantic chunking threshold)
   - similarity > 0.9 (redundancy threshold)
   - T â† if var(S) < Ïµ then Î¼ + Ïƒ else Î¼ (dynamic threshold)
   - 0.5 BM25 + 0.5 LLM (hybrid weights)
   - rerank-english-v3.0 (Cohere model)
```

---

## Checklist para RevisiÃ³n del Paper

### âœ… TÃ©cnicas Implementadas

- [ ] PÃ¡gina 3: Semantic Chunking completo
- [ ] PÃ¡gina 4: Multi-stage scoring description
- [ ] PÃ¡gina 5, LÃ­neas 11-17: Multi-stage scoring algorithm
- [ ] PÃ¡gina 5, LÃ­nea 21: Dynamic threshold formula
- [ ] PÃ¡gina 10: Prompts en Appendix

### âŒ TÃ©cnicas NO Implementadas

- [ ] PÃ¡gina 2, SecciÃ³n 2.4: Redundancy description
- [ ] PÃ¡gina 5, LÃ­neas 4-10: Redundancy algorithm
- [ ] PÃ¡gina 4: Hybrid retrieval description
- [ ] PÃ¡gina 5, LÃ­nea 3: Hybrid retrieval algorithm
- [ ] PÃ¡gina 4: Cohere reranking description
- [ ] PÃ¡gina 5, LÃ­nea 24: Cohere reranking algorithm

### ğŸ“Š Figuras y Tablas

- [ ] Figure 1 (pÃ¡gina 1): Visual comparison
- [ ] Figure 2 (pÃ¡gina 3): Complete pipeline
- [ ] Figure 3 (pÃ¡gina 6): Redundancy effectiveness
- [ ] Table 1 (pÃ¡gina 7): Accuracy results
- [ ] Table 2 (pÃ¡gina 7): Threshold analysis

---

## Citas para PresentaciÃ³n/Defensa

### Para Explicar tu ImplementaciÃ³n âœ…

```
"Como se describe en la secciÃ³n 3 del paper (pÃ¡gina 3), implementamos
Semantic Chunking usando un threshold de Î¸ = 0.8 para agrupar oraciones
consecutivas basÃ¡ndonos en similitud coseno."

"Siguiendo el Algorithm 1 (pÃ¡gina 5, lÃ­neas 11-17), implementamos el
Multi-stage Relevance Scoring con tres etapas: LLMRelevance,
SelfReflect y CriticEval, combinadas mediante pesos 0.3-0.3-0.4."

"El Dynamic Thresholding se implementÃ³ segÃºn la lÃ­nea 21 del Algorithm 1:
T â† if var(S) < Ïµ then Î¼ + Ïƒ else Î¼, que adapta el umbral segÃºn la
distribuciÃ³n de scores."
```

### Para Justificar lo No Implementado âŒ

```
"El paper menciona en la secciÃ³n 2.4 (pÃ¡gina 2) y el Algorithm 1
(lÃ­neas 4-10) la tÃ©cnica de Redundancy Removal con threshold > 0.9,
pero no la implementamos debido a [razÃ³n: tiempo/recursos/prioridades]."

"El Hybrid Retrieval combinando BM25 y LLM con pesos 0.5 (pÃ¡gina 4)
no fue implementado porque [razÃ³n]."

"El Cohere Reranking (rerank-english-v3.0, pÃ¡gina 4) requiere API
de pago y no fue priorizado en esta fase del proyecto."
```

---

## Palabras Clave del Paper (Ctrl+F)

Para buscar rÃ¡pidamente en el PDF:

| TÃ©rmino | Apariciones | PÃ¡ginas Principales |
|---------|-------------|---------------------|
| "semantic chunking" | ~10 | 2, 3, 6 |
| "multi-stage" | ~5 | 4, 5, 6 |
| "dynamic threshold" | ~8 | 4, 5, 7 |
| "redundancy" | ~12 | 2, 4, 6 |
| "BM25" | ~4 | 4, 5 |
| "Cohere" | ~3 | 4, 5 |
| "Î¸ = 0.8" | ~3 | 3, 7 |
| "similarity > 0.9" | ~2 | 2, 4 |
| "rerank-english-v3.0" | 1 | 4 |

---

**Resumen Final**:
- **PÃ¡gina 5 (Algorithm 1)** es la MÃS IMPORTANTE - contiene todas las tÃ©cnicas
- **PÃ¡ginas 3-4** tienen las descripciones metodolÃ³gicas detalladas
- **PÃ¡gina 10 (Appendix)** tiene los prompts exactos para implementaciÃ³n
