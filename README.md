# RAG Engine - ChunkRAG Implementation

Sistema de Retrieval-Augmented Generation (RAG) optimizado con implementaci√≥n del paper **ChunkRAG** (arXiv:2410.19572v5).

**‚ú® Caracter√≠sticas principales:**
- Semantic chunking (Œ∏=0.8)
- Multi-stage LLM filtering con procesamiento async paralelo
- Pipeline de producci√≥n listo para usar
- 4x m√°s r√°pido que implementaci√≥n secuencial

## üìÅ Estructura del Proyecto

```
rag-engine/
‚îú‚îÄ‚îÄ data/                          # PDFs originales
‚îÇ   ‚îú‚îÄ‚îÄ Propuesta_Tecnica_Analitica_Avanzada.pdf
‚îÇ   ‚îî‚îÄ‚îÄ TDD - Learning Journey BCP v2 - 20250429.pdf
‚îÇ
‚îú‚îÄ‚îÄ pipelines/                     # üÜï Pipelines de producci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ production/
‚îÇ       ‚îú‚îÄ‚îÄ rag.py                # Pipeline optimizado (semantic + async filtering)
‚îÇ       ‚îî‚îÄ‚îÄ config.py             # Configuraci√≥n de producci√≥n
‚îÇ
‚îú‚îÄ‚îÄ src/                           # C√≥digo fuente base
‚îÇ   ‚îú‚îÄ‚îÄ chunking/                  # Estrategias de chunking
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chonkie_chunk.py      # Token-based (RecursiveChunker)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ semantic_chunk.py     # Semantic chunking (ChunkRAG)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ filtering/                 # ChunkRAG multi-stage LLM filtering
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chunk_filter.py       # Base ‚Üí Self-Reflection ‚Üí Critic (async)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ retrieval/                 # Query y recuperaci√≥n
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ query.py              # Retrieval con/sin filtering
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ evaluation/                # Evaluaci√≥n con RAGAS
‚îÇ       ‚îú‚îÄ‚îÄ eval_ragas.py         # Pipeline de evaluaci√≥n
‚îÇ       ‚îî‚îÄ‚îÄ ground_truth.py       # Respuestas de referencia
‚îÇ
‚îú‚îÄ‚îÄ scripts/                       # Scripts de ejecuci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ ingest.py                 # Ingesta de PDFs a Qdrant
‚îÇ
‚îú‚îÄ‚îÄ results/                       # Resultados de evaluaciones
‚îÇ   ‚îú‚îÄ‚îÄ baseline/                 # Sin filtering
‚îÇ   ‚îî‚îÄ‚îÄ filtered/                 # Con ChunkRAG filtering
‚îÇ
‚îú‚îÄ‚îÄ config.py                      # Configuraci√≥n (variables de entorno)
‚îú‚îÄ‚îÄ qdrant_client_wrapper.py      # Cliente de Qdrant
‚îú‚îÄ‚îÄ requirements.txt               # Dependencias Python
‚îú‚îÄ‚îÄ pyproject.toml                 # Configuraci√≥n del proyecto
‚îî‚îÄ‚îÄ README.md                      # Este archivo
```

## üöÄ Instalaci√≥n

```bash
# Instalar dependencias
uv sync

# O con pip
pip install -r requirements.txt
```

## üìù Configuraci√≥n

Crear archivo `.env`:

```env
QDRANT_URL=tu_qdrant_url
QDRANT_API_KEY=tu_api_key
OPENAI_API_KEY=tu_openai_key
```

## üîß Uso

### 1. Ingesta de Documentos

```bash
# Ejecutar ingesta (crea colecciones en Qdrant)
uv run scripts/ingest.py
```

Esto crea dos colecciones:
- `benchmark_chonkie`: Chunks basados en tokens (RecursiveChunker)
- `benchmark_semantic`: Chunks sem√°nticos (SemanticChunker con threshold=0.8)

### 2. Pipeline de Producci√≥n (Recomendado) üöÄ

```bash
# Desde terminal
uv run python pipelines/production/rag.py "¬øCu√°l es la arquitectura de Belcorp?"
```

```python
# Desde c√≥digo Python
from pipelines.production import ProductionRAG

rag = ProductionRAG()
result = rag.query("¬øQu√© componentes usa el sistema?", return_chunks=True)

print(result['answer'])
print(f"Chunks usados: {result['num_chunks']}")
```

**Configuraci√≥n del pipeline de producci√≥n:**
- Chunking: `semantic` (Œ∏=0.8)
- Filtering: `enabled` (multi-stage LLM async)
- Retrieval: 15 candidatos ‚Üí 5 filtrados
- Velocidad: ~10-15 segundos por query
- Modelo: `gpt-4o-mini`

### 3. Evaluaci√≥n (Benchmarking)

#### Baseline (sin filtering)
```bash
uv run src/evaluation/eval_ragas.py
```

#### Con ChunkRAG Filtering
```bash
uv run src/evaluation/eval_ragas.py --filter
```

Los resultados se guardan en:
- `results/baseline/` - Evaluaciones sin filtering
- `results/filtered/` - Evaluaciones con multi-stage LLM filtering

## üìä Resultados

### Mejores Configuraciones

**üèÜ GANADOR: Semantic Baseline**
```
Context Recall:       0.703
Faithfulness:         0.993
Factual Correctness:  0.398 (60% mejor que Chonkie)
```

### Comparativa Completa

| Configuraci√≥n | Context Recall | Faithfulness | Factual Correctness |
|---------------|----------------|--------------|---------------------|
| Chonkie Baseline | 1.000 | 0.983 | 0.248 |
| Chonkie Filtered | 1.000 | 1.000 | 0.314 (+26.6%) |
| **Semantic Baseline** | **0.703** | **0.993** | **0.398** üèÜ |
| Semantic Filtered | 0.689 | 1.000 | 0.354 |

### Conclusiones

1. **Semantic Chunking > Token-based**: +60% en precisi√≥n factual
2. **Trade-off Recall/Precisi√≥n**: Semantic pierde 30% recall pero gana 60% en precisi√≥n
3. **Filtering en Semantic**: Contraproducente (-11% factual), los chunks ya est√°n bien filtrados
4. **Filtering en Chonkie**: Beneficioso (+26.6% factual), ayuda a limpiar ruido

## üß™ M√©tricas RAGAS

- **Context Recall**: Proporci√≥n de informaci√≥n necesaria recuperada
- **Faithfulness**: Fidelidad al contexto (detecta alucinaciones)
- **Factual Correctness**: Precisi√≥n factual vs ground truth (F1 score)

## üìö Implementaci√≥n ChunkRAG

### T√©cnicas Implementadas (4/7 del paper)

#### ‚úÖ 1. Semantic Chunking
**Archivo:** `src/chunking/semantic_chunk.py`

Basado en el paper (Secci√≥n 3):
- Tokenizaci√≥n por oraciones (NLTK)
- Embeddings: `text-embedding-3-small`
- Threshold de similitud: **Œ∏ = 0.8**
- L√≠mite de chunk: **128 tokens (~500 chars)**
- Agrupaci√≥n por cosine similarity

#### ‚úÖ 2. Multi-stage LLM Filtering (Async Optimizado)
**Archivo:** `src/filtering/chunk_filter.py`

Basado en el paper (Secci√≥n 3.2):

1. **Base Score**: LLM eval√∫a relevancia inicial (0-1)
2. **Self-Reflection**: LLM reflexiona y ajusta score
3. **Critic Evaluation**: Evaluaci√≥n cr√≠tica con heur√≠sticas
4. **Score Final**: `0.3 * base + 0.3 * reflect + 0.4 * critic`

**Optimizaci√≥n async:**
- Procesamiento paralelo de chunks con `asyncio.gather`
- **4x m√°s r√°pido** que versi√≥n secuencial
- Tiempo: ~10-15s vs ~45s

#### ‚úÖ 3. Dynamic Thresholding
**Archivo:** `src/filtering/chunk_filter.py:154-177`

```python
threshold = Œº + œÉ if var < Œµ else Œº
```

Adaptaci√≥n autom√°tica seg√∫n distribuci√≥n de scores.

#### ‚úÖ 4. Chunk-level Filtering
**Archivo:** `src/filtering/chunk_filter.py`

Filtrado granular a nivel de chunk (no documento completo).

### T√©cnicas No Implementadas

- ‚ùå **Redundancy removal** (similitud >0.9)
- ‚ùå **Hybrid retrieval** (BM25 + embeddings)
- ‚ùå **Cohere reranking** (anti "Lost in the middle")

*Nota: El sistema actual funciona bien sin estas optimizaciones adicionales.*

## ‚ö° Performance

### Velocidad de Filtering

| Versi√≥n | Tiempo (15 chunks) | Optimizaci√≥n |
|---------|-------------------|--------------|
| Secuencial | ~45s | Baseline |
| **Async Paralelo** | **~11s** | **4x m√°s r√°pido** |

### M√©tricas de Calidad (RAGAS)

**Pipeline de producci√≥n (semantic + filtering):**
- Context Recall: 0.70
- Faithfulness: 0.99
- Factual Correctness: 0.35

## üîß Configuraci√≥n Avanzada

### Desactivar Async Filtering

```python
from src.filtering import filter_chunks_by_relevance

# Usar versi√≥n secuencial
chunks = filter_chunks_by_relevance(
    chunks,
    query,
    use_async=False  # Secuencial (legacy)
)
```

### Ajustar Par√°metros de Producci√≥n

Editar `pipelines/production/config.py`:

```python
ENABLE_FILTERING = True         # Activar/desactivar filtering
INITIAL_RETRIEVAL_K = 15        # Candidatos iniciales
FINAL_CHUNKS_K = 5              # Chunks finales
SEMANTIC_THRESHOLD = 0.8        # Umbral de similitud
```

## üîó Referencias

- Paper: [ChunkRAG (arXiv:2410.19572v5)](https://arxiv.org/abs/2410.19572)
- Chonkie: [https://docs.chonkie.ai](https://docs.chonkie.ai)
- RAGAS: [https://docs.ragas.io](https://docs.ragas.io)
- OpenAI Embeddings: [text-embedding-3-large](https://platform.openai.com/docs/guides/embeddings)

## üìÑ Licencia

Proyecto educacional/investigaci√≥n.
