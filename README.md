# RAG Engine - ChunkRAG Implementation

Sistema de Retrieval-Augmented Generation (RAG) con implementaci√≥n completa del paper **ChunkRAG** (arXiv:2410.19572v5).

## üìÅ Estructura del Proyecto

```
rag-engine/
‚îú‚îÄ‚îÄ data/                          # PDFs originales
‚îÇ   ‚îú‚îÄ‚îÄ Propuesta_Tecnica_Analitica_Avanzada.pdf
‚îÇ   ‚îî‚îÄ‚îÄ TDD - Learning Journey BCP v2 - 20250429.pdf
‚îÇ
‚îú‚îÄ‚îÄ src/                           # C√≥digo fuente principal
‚îÇ   ‚îú‚îÄ‚îÄ chunking/                  # Estrategias de chunking
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chonkie_chunk.py      # Token-based (RecursiveChunker)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ semantic_chunk.py     # Semantic chunking (ChunkRAG)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ filtering/                 # ChunkRAG multi-stage LLM filtering
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chunk_filter.py       # Base ‚Üí Self-Reflection ‚Üí Critic
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ retrieval/                 # Query y recuperaci√≥n
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ query.py              # Retrieval con/sin filtering
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ evaluation/                # Evaluaci√≥n con RAGAS
‚îÇ       ‚îú‚îÄ‚îÄ eval_ragas.py         # Pipeline de evaluaci√≥n
‚îÇ       ‚îî‚îÄ‚îÄ ground_truth.py       # Respuestas de referencia
‚îÇ
‚îú‚îÄ‚îÄ scripts/                       # Scripts de ejecuci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ ingest.py                 # Ingesta de PDFs a Qdrant
‚îÇ
‚îú‚îÄ‚îÄ results/                       # Resultados de evaluaciones
‚îÇ   ‚îú‚îÄ‚îÄ baseline/                 # Sin filtering
‚îÇ   ‚îî‚îÄ‚îÄ filtered/                 # Con ChunkRAG filtering
‚îÇ
‚îú‚îÄ‚îÄ config.py                      # Configuraci√≥n (variables de entorno)
‚îú‚îÄ‚îÄ qdrant_client_wrapper.py      # Cliente de Qdrant
‚îú‚îÄ‚îÄ requirements.txt               # Dependencias Python
‚îú‚îÄ‚îÄ pyproject.toml                 # Configuraci√≥n del proyecto
‚îî‚îÄ‚îÄ README.md                      # Este archivo
```

## üöÄ Instalaci√≥n

```bash
# Instalar dependencias
uv sync

# O con pip
pip install -r requirements.txt
```

## üìù Configuraci√≥n

Crear archivo `.env`:

```env
QDRANT_URL=tu_qdrant_url
QDRANT_API_KEY=tu_api_key
OPENAI_API_KEY=tu_openai_key
```

## üîß Uso

### 1. Ingesta de Documentos

```bash
# Ejecutar ingesta (crea colecciones en Qdrant)
uv run scripts/ingest.py
```

Esto crea dos colecciones:
- `benchmark_chonkie`: Chunks basados en tokens (RecursiveChunker)
- `benchmark_semantic`: Chunks sem√°nticos (SemanticChunker con threshold=0.8)

### 2. Evaluaci√≥n

#### Baseline (sin filtering)
```bash
uv run src/evaluation/eval_ragas.py
```

#### Con ChunkRAG Filtering
```bash
uv run src/evaluation/eval_ragas.py --filter
```

Los resultados se guardan en:
- `results/baseline/` - Evaluaciones sin filtering
- `results/filtered/` - Evaluaciones con multi-stage LLM filtering

## üìä Resultados

### Mejores Configuraciones

**üèÜ GANADOR: Semantic Baseline**
```
Context Recall:       0.703
Faithfulness:         0.993
Factual Correctness:  0.398 (60% mejor que Chonkie)
```

### Comparativa Completa

| Configuraci√≥n | Context Recall | Faithfulness | Factual Correctness |
|---------------|----------------|--------------|---------------------|
| Chonkie Baseline | 1.000 | 0.983 | 0.248 |
| Chonkie Filtered | 1.000 | 1.000 | 0.314 (+26.6%) |
| **Semantic Baseline** | **0.703** | **0.993** | **0.398** üèÜ |
| Semantic Filtered | 0.689 | 1.000 | 0.354 |

### Conclusiones

1. **Semantic Chunking > Token-based**: +60% en precisi√≥n factual
2. **Trade-off Recall/Precisi√≥n**: Semantic pierde 30% recall pero gana 60% en precisi√≥n
3. **Filtering en Semantic**: Contraproducente (-11% factual), los chunks ya est√°n bien filtrados
4. **Filtering en Chonkie**: Beneficioso (+26.6% factual), ayuda a limpiar ruido

## üß™ M√©tricas RAGAS

- **Context Recall**: Proporci√≥n de informaci√≥n necesaria recuperada
- **Faithfulness**: Fidelidad al contexto (detecta alucinaciones)
- **Factual Correctness**: Precisi√≥n factual vs ground truth (F1 score)

## üìö Implementaci√≥n ChunkRAG

### Semantic Chunking

Basado en el paper (Secci√≥n 3):
- Tokenizaci√≥n por oraciones (NLTK)
- Embeddings: `text-embedding-3-small`
- Threshold de similitud: **Œ∏ = 0.8**
- L√≠mite de chunk: **128 tokens (~500 chars)**
- Agrupaci√≥n por cosine similarity

### Multi-stage LLM Filtering

Basado en el paper (Secci√≥n 3.2):

1. **Base Score**: LLM eval√∫a relevancia inicial (0-1)
2. **Self-Reflection**: LLM reflexiona y ajusta score
3. **Critic Evaluation**: Evaluaci√≥n cr√≠tica con heur√≠sticas
4. **Score Final**: `0.3 * base + 0.3 * reflect + 0.4 * critic`
5. **Dynamic Thresholding**: `threshold = mean + std if var < Œµ else mean`

## üîó Referencias

- Paper: [ChunkRAG (arXiv:2410.19572v5)](https://arxiv.org/abs/2410.19572)
- Chonkie: [https://docs.chonkie.ai](https://docs.chonkie.ai)
- RAGAS: [https://docs.ragas.io](https://docs.ragas.io)

## üìÑ Licencia

Proyecto educacional/investigaci√≥n.
