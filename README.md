# RAG Engine - ChunkRAG Implementation

Sistema de Retrieval-Augmented Generation (RAG) optimizado con implementaci√≥n del paper **ChunkRAG** (arXiv:2410.19572v5).

**‚ú® Caracter√≠sticas principales:**
- Semantic chunking (Œ∏=0.8)
- Multi-stage LLM filtering con procesamiento paralelo
- **Cohere Rerank v3.5** para reordenamiento √≥ptimo (nuevo)
- Pipeline de producci√≥n listo para usar
- 4x m√°s r√°pido que implementaci√≥n secuencial
- **DSPy integration** para optimizaci√≥n autom√°tica de prompts
- **5 de 7 t√©cnicas del paper ChunkRAG implementadas**

## üìÅ Estructura del Proyecto

```
rag-engine/
‚îú‚îÄ‚îÄ data/                          # PDFs originales
‚îÇ   ‚îú‚îÄ‚îÄ Propuesta_Tecnica_Analitica_Avanzada.pdf
‚îÇ   ‚îî‚îÄ‚îÄ TDD - Learning Journey BCP v2 - 20250429.pdf
‚îÇ
‚îú‚îÄ‚îÄ docs/                          # üìö Documentaci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ DSPY_IMPLEMENTATION.md    # Gu√≠a completa de DSPy
‚îÇ
‚îú‚îÄ‚îÄ pipelines/                     # üÜï Pipelines de producci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ production/
‚îÇ       ‚îú‚îÄ‚îÄ rag.py                # Pipeline original (async filtering)
‚îÇ       ‚îú‚îÄ‚îÄ rag_dspy.py           # Pipeline DSPy (prompts optimizables)
‚îÇ       ‚îú‚îÄ‚îÄ rag_cohere_rerank.py  # üÜï Pipeline con Cohere Rerank v3.5
‚îÇ       ‚îú‚îÄ‚îÄ rag_only_rerank.py    # Pipeline solo con reranking (comparaci√≥n)
‚îÇ       ‚îú‚îÄ‚îÄ config.py             # Configuraci√≥n de producci√≥n
‚îÇ       ‚îî‚îÄ‚îÄ compiled_scorer.json  # Scorer DSPy optimizado (generado)
‚îÇ
‚îú‚îÄ‚îÄ src/                           # C√≥digo fuente base
‚îÇ   ‚îú‚îÄ‚îÄ chunking/                  # Estrategias de chunking
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ semantic_chunk.py     # Semantic chunking (ChunkRAG)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ filtering/                 # ChunkRAG multi-stage LLM filtering
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chunk_filter.py       # Base ‚Üí Self-Reflection ‚Üí Critic (async)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chunk_filter_dspy.py  # Versi√≥n DSPy optimizable (threads)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ reranker.py           # üÜï Cohere Rerank v3.5 integration
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ retrieval/                 # Query y recuperaci√≥n
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ query.py              # Retrieval con/sin filtering
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ evaluation/                # Evaluaci√≥n con RAGAS
‚îÇ       ‚îú‚îÄ‚îÄ eval_ragas.py         # Pipeline de evaluaci√≥n
‚îÇ       ‚îî‚îÄ‚îÄ ground_truth.py       # Respuestas de referencia
‚îÇ
‚îú‚îÄ‚îÄ scripts/                       # Scripts de ejecuci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ ingest.py                 # Ingesta de PDFs a Qdrant (semantic only)
‚îÇ   ‚îî‚îÄ‚îÄ train_dspy.py             # Entrenamiento DSPy con ejemplos
‚îÇ
‚îú‚îÄ‚îÄ results/                       # Resultados de evaluaciones
‚îÇ   ‚îú‚îÄ‚îÄ baseline/                 # Sin filtering
‚îÇ   ‚îî‚îÄ‚îÄ filtered/                 # Con ChunkRAG filtering
‚îÇ
‚îú‚îÄ‚îÄ config.py                      # Configuraci√≥n (variables de entorno)
‚îú‚îÄ‚îÄ qdrant_client_wrapper.py      # Cliente de Qdrant
‚îú‚îÄ‚îÄ requirements.txt               # Dependencias Python
‚îú‚îÄ‚îÄ pyproject.toml                 # Configuraci√≥n del proyecto
‚îî‚îÄ‚îÄ README.md                      # Este archivo
```

## üöÄ Instalaci√≥n

```bash
# Instalar dependencias
uv sync

# O con pip
pip install -r requirements.txt
```

## üìù Configuraci√≥n

Crear archivo `.env`:

```env
QDRANT_URL=tu_qdrant_url
QDRANT_API_KEY=tu_api_key
OPENAI_API_KEY=tu_openai_key
```

## üîß Uso

### 1. Ingesta de Documentos

```bash
# Ejecutar ingesta (crea colecci√≥n en Qdrant)
uv run scripts/ingest.py
```

Esto crea la colecci√≥n:
- `benchmark_semantic`: Chunks sem√°nticos (SemanticChunker con threshold=0.8)

*Nota: Chonkie chunking est√° deshabilitado. Solo se usa semantic chunking para producci√≥n.*

### 2. Pipeline de Producci√≥n (Recomendado) üöÄ

#### Opci√≥n A: Pipeline Original (AsyncIO)

```bash
# Desde terminal
uv run python pipelines/production/rag.py "¬øCu√°l es la arquitectura de Belcorp?"
```

```python
# Desde c√≥digo Python
from pipelines.production import ProductionRAG

rag = ProductionRAG()
result = rag.query("¬øQu√© componentes usa el sistema?", return_chunks=True)

print(result['answer'])
print(f"Chunks usados: {result['num_chunks']}")
```

**Configuraci√≥n:**
- Chunking: `semantic` (Œ∏=0.8)
- Filtering: `enabled` (multi-stage LLM async)
- Retrieval: 15 candidatos ‚Üí 5 filtrados
- Velocidad: ~2-3 segundos por query
- Modelo: `gpt-4o-mini`

#### Opci√≥n B: Pipeline DSPy (Prompts Optimizables) ü§ñ

```bash
# Entrenar scorer DSPy (solo una vez, toma 2-5 minutos)
uv run python scripts/train_dspy.py

# Usar pipeline DSPy (carga autom√°ticamente el scorer optimizado)
uv run python pipelines/production/rag_dspy.py "¬øCu√°l es la arquitectura de Belcorp?"
```

```python
# Desde c√≥digo Python
from pipelines.production import ProductionRAGDSPy

rag = ProductionRAGDSPy()  # Auto-carga compiled_scorer.json si existe
result = rag.query("¬øQu√© componentes usa el sistema?", return_chunks=True)

print(result['answer'])
print(f"Chunks usados: {result['num_chunks']}")
```

**Configuraci√≥n:**
- Chunking: `semantic` (Œ∏=0.8)
- Filtering: `DSPy optimizable` (multi-stage con threads)
- Retrieval: 15 candidatos ‚Üí 5 filtrados
- Velocidad: ~5-10 segundos por query
- Modelo: `gpt-4o-mini`

#### Opci√≥n C: Pipeline con Cohere Rerank v3.5 (M√°xima Accuracy) ‚≠ê

```bash
# Pipeline completo con reranking (5 de 7 t√©cnicas del paper)
uv run python pipelines/production/rag_cohere_rerank.py "¬øCu√°l es la arquitectura de Belcorp?"
```

```python
# Desde c√≥digo Python
from pipelines.production import CohereRerankRAG

# Configuraci√≥n completa (recomendada)
rag = CohereRerankRAG(use_filtering=True, use_reranking=True)
result = rag.query("¬øQu√© componentes usa el sistema?", return_chunks=True)

print(result['answer'])
print(f"Chunks usados: {result['num_chunks']}")
print(f"Filtering: {result['filtering_enabled']}")
print(f"Reranking: {result['reranking_enabled']}")
```

**Configuraci√≥n:**
- Chunking: `chonkie` (semantic, Œ∏=0.8)
- Filtering: `enabled` (multi-stage LLM async)
- Reranking: `Cohere Rerank v3.5` (AWS Bedrock)
- Pipeline: Vector ‚Üí Multi-stage filtering ‚Üí Cohere rerank ‚Üí Generation
- Retrieval: 15 candidatos ‚Üí 5 filtrados ‚Üí 5 reordenados
- Velocidad: ~17-48 segundos por query
- Costo: ~$0.002 por query (solo Cohere)
- Accuracy esperada: **64.9%** (seg√∫n paper ChunkRAG)
- Modelo: `gpt-4o-mini`

**Requisitos:**
- AWS credentials configuradas (`aws configure`)
- Permisos de Bedrock en regi√≥n `us-east-1`
- Dependencia: `boto3` (instalar con `uv pip install boto3`)

üìö **Para m√°s detalles sobre DSPy, ver:** [`docs/DSPY_IMPLEMENTATION.md`](docs/DSPY_IMPLEMENTATION.md)

### 3. Evaluaci√≥n (Benchmarking)

#### Baseline (sin filtering)
```bash
uv run src/evaluation/eval_ragas.py
```

#### Con ChunkRAG Filtering
```bash
uv run src/evaluation/eval_ragas.py --filter
```

Los resultados se guardan en:
- `results/baseline/` - Evaluaciones sin filtering
- `results/filtered/` - Evaluaciones con multi-stage LLM filtering

## üìä Resultados

### Mejores Configuraciones

**üèÜ GANADOR: Semantic Baseline**
```
Context Recall:       0.703
Faithfulness:         0.993
Factual Correctness:  0.398 (60% mejor que Chonkie)
```

### Comparativa Completa

| Configuraci√≥n | Context Recall | Faithfulness | Factual Correctness |
|---------------|----------------|--------------|---------------------|
| Chonkie Baseline | 1.000 | 0.983 | 0.248 |
| Chonkie Filtered | 1.000 | 1.000 | 0.314 (+26.6%) |
| **Semantic Baseline** | **0.703** | **0.993** | **0.398** üèÜ |
| Semantic Filtered | 0.689 | 1.000 | 0.354 |

### Conclusiones

1. **Semantic Chunking > Token-based**: +60% en precisi√≥n factual
2. **Trade-off Recall/Precisi√≥n**: Semantic pierde 30% recall pero gana 60% en precisi√≥n
3. **Filtering en Semantic**: Contraproducente (-11% factual), los chunks ya est√°n bien filtrados
4. **Filtering en Chonkie**: Beneficioso (+26.6% factual), ayuda a limpiar ruido

## üß™ M√©tricas RAGAS

- **Context Recall**: Proporci√≥n de informaci√≥n necesaria recuperada
- **Faithfulness**: Fidelidad al contexto (detecta alucinaciones)
- **Factual Correctness**: Precisi√≥n factual vs ground truth (F1 score)

## üìö Implementaci√≥n ChunkRAG

### T√©cnicas Implementadas (5/7 del paper)

#### ‚úÖ 1. Semantic Chunking
**Archivo:** `src/chunking/semantic_chunk.py`

Basado en el paper (Secci√≥n 3):
- Tokenizaci√≥n por oraciones (NLTK)
- Embeddings: `text-embedding-3-small`
- Threshold de similitud: **Œ∏ = 0.8**
- L√≠mite de chunk: **128 tokens (~500 chars)**
- Agrupaci√≥n por cosine similarity

#### ‚úÖ 2. Multi-stage LLM Filtering (Async Optimizado)
**Archivo:** `src/filtering/chunk_filter.py`

Basado en el paper (Secci√≥n 3.2):

1. **Base Score**: LLM eval√∫a relevancia inicial (0-1)
2. **Self-Reflection**: LLM reflexiona y ajusta score
3. **Critic Evaluation**: Evaluaci√≥n cr√≠tica con heur√≠sticas
4. **Score Final**: `0.3 * base + 0.3 * reflect + 0.4 * critic`

**Optimizaci√≥n async:**
- Procesamiento paralelo de chunks con `asyncio.gather`
- **4x m√°s r√°pido** que versi√≥n secuencial
- Tiempo: ~10-15s vs ~45s

#### ‚úÖ 3. Dynamic Thresholding
**Archivo:** `src/filtering/chunk_filter.py:154-177`

```python
threshold = Œº + œÉ if var < Œµ else Œº
```

Adaptaci√≥n autom√°tica seg√∫n distribuci√≥n de scores.

#### ‚úÖ 4. Chunk-level Filtering
**Archivo:** `src/filtering/chunk_filter.py`

Filtrado granular a nivel de chunk (no documento completo).

#### ‚úÖ 5. Cohere Rerank v3.5 (NUEVO)
**Archivo:** `src/filtering/reranker.py`

Basado en el paper (Algorithm 1, l√≠neas 23-24):
- Modelo: `cohere.rerank-v3-5:0` via AWS Bedrock
- Reordena chunks filtrados por relevancia
- Resuelve problema "Lost in the Middle"
- Costo: $2.00 por 1,000 queries (~$0.002 por query)
- Integraci√≥n: Pipeline `pipelines/production/rag_cohere_rerank.py`

**Formato de request:**
```python
{
  "query": "user query",
  "documents": ["chunk1", "chunk2", ...],
  "top_n": 5,
  "api_version": 2
}
```

### T√©cnicas No Implementadas (2/7)

- ‚ùå **Redundancy removal** (similitud >0.9) - Paper secci√≥n 2.4
- ‚ùå **Hybrid retrieval** (BM25 + embeddings) - Paper Algorithm 1, l√≠neas 2-3

*Nota: Con 5/7 t√©cnicas implementadas, el sistema alcanza el 71% de la configuraci√≥n completa del paper.*

## ‚ö° Performance

### Velocidad de Filtering

| Versi√≥n | Tiempo (15 chunks) | Optimizaci√≥n |
|---------|-------------------|--------------|
| Secuencial | ~45s | Baseline |
| **Async Paralelo** | **~11s** | **4x m√°s r√°pido** |

### M√©tricas de Calidad (RAGAS)

**Pipeline de producci√≥n (semantic + filtering):**
- Context Recall: 0.70
- Faithfulness: 0.99
- Factual Correctness: 0.35

## ü§ñ DSPy: Optimizaci√≥n Autom√°tica de Prompts

**DSPy** (Declarative Self-improving Language Programs) es un framework de Stanford que permite **optimizar prompts autom√°ticamente** en lugar de escribirlos manualmente.

### ¬øPor qu√© DSPy?

El sistema usa **3 prompts diferentes** para filtering multi-etapa (Base, Reflection, Critic). Tradicionalmente:
- ‚ùå Escribir prompts manualmente
- ‚ùå Ajustar por prueba y error
- ‚ùå Dif√≠cil de mejorar sistem√°ticamente

Con DSPy:
- ‚úÖ Prompts optimizables autom√°ticamente
- ‚úÖ Entrenamiento con ejemplos
- ‚úÖ Mejora continua agregando datos

### Diferencia Clave: AsyncIO vs Threads

| Aspecto | Pipeline Original | Pipeline DSPy |
|---------|------------------|---------------|
| Procesamiento | AsyncIO | ThreadPoolExecutor |
| Velocidad | ~2-3s | ~3-4s |
| Prompts | Hardcoded | Optimizables |
| Paralelismo | `asyncio.gather` | `ThreadPoolExecutor(max_workers=15)` |

**¬øPor qu√© threads en DSPy?**

DSPy usa llamadas s√≠ncronas a OpenAI internamente, por lo que `asyncio` no funciona. La soluci√≥n es `ThreadPoolExecutor`:

```python
# Original (AsyncIO)
async def score_chunk(chunk, query):
    response = await llm.ainvoke(prompt)  # Async nativo
    return score

results = await asyncio.gather(*tasks)  # Paralelo

# DSPy (Threads)
def score_chunk_dspy(scorer, chunk, query):
    scores = scorer(chunk=chunk, query=query)  # Sync
    return scores

with ThreadPoolExecutor(max_workers=15) as executor:
    futures = [executor.submit(score_chunk_dspy, ...) for chunk in chunks]
    results = [f.result() for f in as_completed(futures)]  # Paralelo
```

Ambos m√©todos logran **paralelismo real**, pero con diferentes mecanismos internos.

### Entrenamiento DSPy

```bash
# Entrenar con 8 ejemplos (high/medium/low relevance)
uv run python scripts/train_dspy.py

# Salida:
# - Bootstrapped 4 full traces
# - 2 rondas de optimizaci√≥n
# - Genera: pipelines/production/compiled_scorer.json
```

El archivo `compiled_scorer.json` (10 KB) contiene:
- Prompts optimizados
- Ejemplos few-shot seleccionados
- Configuraci√≥n del scorer

### Comparaci√≥n de Resultados

**Mismo query:** "¬øCu√°l es la arquitectura de Belcorp?"

| M√©trica | Pipeline Original | DSPy Optimizado |
|---------|------------------|-----------------|
| Tiempo | 2s | 3s |
| Chunks filtrados | 6 | 7 |
| Threshold | 0.389 | 0.307 |

**Chunks m√°s relevantes (ambos incluyen):**
- Propuesta T√©cnica Plataforma de Anal√≠tica
- Reutilizar activos tecnol√≥gicos
- Componentes modulares

**Observaci√≥n:** DSPy selecciona chunks ligeramente diferentes pero relevantes. Requiere evaluaci√≥n con RAGAS para validar calidad.

### Pr√≥ximos Pasos con DSPy

1. **Evaluar con RAGAS**: Comparar m√©tricas vs pipeline original
2. **Ampliar dataset**: Agregar m√°s ejemplos (50-100)
3. **Experimentar con 2 etapas**: Quiz√°s Base + Critic sea suficiente
4. **Optimizadores avanzados**: MIPROv2, COPRO

üìö **Documentaci√≥n completa:** [`docs/DSPY_IMPLEMENTATION.md`](docs/DSPY_IMPLEMENTATION.md)

## üîß Configuraci√≥n Avanzada

### Desactivar Async Filtering

```python
from src.filtering import filter_chunks_by_relevance

# Usar versi√≥n secuencial
chunks = filter_chunks_by_relevance(
    chunks,
    query,
    use_async=False  # Secuencial (legacy)
)
```

### Ajustar Par√°metros de Producci√≥n

Editar `pipelines/production/config.py`:

```python
ENABLE_FILTERING = True         # Activar/desactivar filtering
INITIAL_RETRIEVAL_K = 15        # Candidatos iniciales
FINAL_CHUNKS_K = 5              # Chunks finales
SEMANTIC_THRESHOLD = 0.8        # Umbral de similitud
```

## üîó Referencias

- Paper: [ChunkRAG (arXiv:2410.19572v5)](https://arxiv.org/abs/2410.19572)
- Chonkie: [https://docs.chonkie.ai](https://docs.chonkie.ai)
- RAGAS: [https://docs.ragas.io](https://docs.ragas.io)
- OpenAI Embeddings: [text-embedding-3-large](https://platform.openai.com/docs/guides/embeddings)

## üìÑ Licencia

Proyecto educacional/investigaci√≥n.
