# RAG Engine - ChunkRAG Implementation

Sistema de Retrieval-Augmented Generation (RAG) optimizado con implementaci√≥n del paper **ChunkRAG** (arXiv:2410.19572v5).

**‚ú® Caracter√≠sticas principales:**
- Semantic chunking (Œ∏=0.8)
- Multi-stage LLM filtering con procesamiento paralelo
- Pipeline de producci√≥n listo para usar
- 4x m√°s r√°pido que implementaci√≥n secuencial
- **DSPy integration** para optimizaci√≥n autom√°tica de prompts

## üìÅ Estructura del Proyecto

```
rag-engine/
‚îú‚îÄ‚îÄ data/                          # PDFs originales
‚îÇ   ‚îú‚îÄ‚îÄ Propuesta_Tecnica_Analitica_Avanzada.pdf
‚îÇ   ‚îî‚îÄ‚îÄ TDD - Learning Journey BCP v2 - 20250429.pdf
‚îÇ
‚îú‚îÄ‚îÄ docs/                          # üìö Documentaci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ DSPY_IMPLEMENTATION.md    # Gu√≠a completa de DSPy
‚îÇ
‚îú‚îÄ‚îÄ pipelines/                     # üÜï Pipelines de producci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ production/
‚îÇ       ‚îú‚îÄ‚îÄ rag.py                # Pipeline original (async filtering)
‚îÇ       ‚îú‚îÄ‚îÄ rag_dspy.py           # Pipeline DSPy (prompts optimizables)
‚îÇ       ‚îú‚îÄ‚îÄ config.py             # Configuraci√≥n de producci√≥n
‚îÇ       ‚îî‚îÄ‚îÄ compiled_scorer.json  # Scorer DSPy optimizado (generado)
‚îÇ
‚îú‚îÄ‚îÄ src/                           # C√≥digo fuente base
‚îÇ   ‚îú‚îÄ‚îÄ chunking/                  # Estrategias de chunking
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ semantic_chunk.py     # Semantic chunking (ChunkRAG)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ filtering/                 # ChunkRAG multi-stage LLM filtering
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chunk_filter.py       # Base ‚Üí Self-Reflection ‚Üí Critic (async)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chunk_filter_dspy.py  # Versi√≥n DSPy optimizable (threads)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ retrieval/                 # Query y recuperaci√≥n
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ query.py              # Retrieval con/sin filtering
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ evaluation/                # Evaluaci√≥n con RAGAS
‚îÇ       ‚îú‚îÄ‚îÄ eval_ragas.py         # Pipeline de evaluaci√≥n
‚îÇ       ‚îî‚îÄ‚îÄ ground_truth.py       # Respuestas de referencia
‚îÇ
‚îú‚îÄ‚îÄ scripts/                       # Scripts de ejecuci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ ingest.py                 # Ingesta de PDFs a Qdrant (semantic only)
‚îÇ   ‚îî‚îÄ‚îÄ train_dspy.py             # Entrenamiento DSPy con ejemplos
‚îÇ
‚îú‚îÄ‚îÄ results/                       # Resultados de evaluaciones
‚îÇ   ‚îú‚îÄ‚îÄ baseline/                 # Sin filtering
‚îÇ   ‚îî‚îÄ‚îÄ filtered/                 # Con ChunkRAG filtering
‚îÇ
‚îú‚îÄ‚îÄ config.py                      # Configuraci√≥n (variables de entorno)
‚îú‚îÄ‚îÄ qdrant_client_wrapper.py      # Cliente de Qdrant
‚îú‚îÄ‚îÄ requirements.txt               # Dependencias Python
‚îú‚îÄ‚îÄ pyproject.toml                 # Configuraci√≥n del proyecto
‚îî‚îÄ‚îÄ README.md                      # Este archivo
```

## üöÄ Instalaci√≥n

```bash
# Instalar dependencias
uv sync

# O con pip
pip install -r requirements.txt
```

## üìù Configuraci√≥n

Crear archivo `.env`:

```env
QDRANT_URL=tu_qdrant_url
QDRANT_API_KEY=tu_api_key
OPENAI_API_KEY=tu_openai_key
```

## üîß Uso

### 1. Ingesta de Documentos

```bash
# Ejecutar ingesta (crea colecci√≥n en Qdrant)
uv run scripts/ingest.py
```

Esto crea la colecci√≥n:
- `benchmark_semantic`: Chunks sem√°nticos (SemanticChunker con threshold=0.8)

*Nota: Chonkie chunking est√° deshabilitado. Solo se usa semantic chunking para producci√≥n.*

### 2. Pipeline de Producci√≥n (Recomendado) üöÄ

#### Opci√≥n A: Pipeline Original (AsyncIO)

```bash
# Desde terminal
uv run python pipelines/production/rag.py "¬øCu√°l es la arquitectura de Belcorp?"
```

```python
# Desde c√≥digo Python
from pipelines.production import ProductionRAG

rag = ProductionRAG()
result = rag.query("¬øQu√© componentes usa el sistema?", return_chunks=True)

print(result['answer'])
print(f"Chunks usados: {result['num_chunks']}")
```

**Configuraci√≥n:**
- Chunking: `semantic` (Œ∏=0.8)
- Filtering: `enabled` (multi-stage LLM async)
- Retrieval: 15 candidatos ‚Üí 5 filtrados
- Velocidad: ~2-3 segundos por query
- Modelo: `gpt-4o-mini`

#### Opci√≥n B: Pipeline DSPy (Prompts Optimizables) ü§ñ

```bash
# Entrenar scorer DSPy (solo una vez, toma 2-5 minutos)
uv run python scripts/train_dspy.py

# Usar pipeline DSPy (carga autom√°ticamente el scorer optimizado)
uv run python pipelines/production/rag_dspy.py "¬øCu√°l es la arquitectura de Belcorp?"
```

```python
# Desde c√≥digo Python
from pipelines.production import ProductionRAGDSPy

rag = ProductionRAGDSPy()  # Auto-carga compiled_scorer.json si existe
result = rag.query("¬øQu√© componentes usa el sistema?", return_chunks=True)

print(result['answer'])
print(f"Chunks usados: {result['num_chunks']}")
```

**Configuraci√≥n:**
- Chunking: `semantic` (Œ∏=0.8)
- Filtering: `DSPy optimizable` (multi-stage con threads)
- Retrieval: 15 candidatos ‚Üí 5 filtrados
- Velocidad: ~3-4 segundos por query
- Modelo: `gpt-4o-mini`
- **Ventaja:** Prompts optimizables autom√°ticamente

üìö **Para m√°s detalles sobre DSPy, ver:** [`docs/DSPY_IMPLEMENTATION.md`](docs/DSPY_IMPLEMENTATION.md)

### 3. Evaluaci√≥n (Benchmarking)

#### Baseline (sin filtering)
```bash
uv run src/evaluation/eval_ragas.py
```

#### Con ChunkRAG Filtering
```bash
uv run src/evaluation/eval_ragas.py --filter
```

Los resultados se guardan en:
- `results/baseline/` - Evaluaciones sin filtering
- `results/filtered/` - Evaluaciones con multi-stage LLM filtering

## üìä Resultados

### Mejores Configuraciones

**üèÜ GANADOR: Semantic Baseline**
```
Context Recall:       0.703
Faithfulness:         0.993
Factual Correctness:  0.398 (60% mejor que Chonkie)
```

### Comparativa Completa

| Configuraci√≥n | Context Recall | Faithfulness | Factual Correctness |
|---------------|----------------|--------------|---------------------|
| Chonkie Baseline | 1.000 | 0.983 | 0.248 |
| Chonkie Filtered | 1.000 | 1.000 | 0.314 (+26.6%) |
| **Semantic Baseline** | **0.703** | **0.993** | **0.398** üèÜ |
| Semantic Filtered | 0.689 | 1.000 | 0.354 |

### Conclusiones

1. **Semantic Chunking > Token-based**: +60% en precisi√≥n factual
2. **Trade-off Recall/Precisi√≥n**: Semantic pierde 30% recall pero gana 60% en precisi√≥n
3. **Filtering en Semantic**: Contraproducente (-11% factual), los chunks ya est√°n bien filtrados
4. **Filtering en Chonkie**: Beneficioso (+26.6% factual), ayuda a limpiar ruido

## üß™ M√©tricas RAGAS

- **Context Recall**: Proporci√≥n de informaci√≥n necesaria recuperada
- **Faithfulness**: Fidelidad al contexto (detecta alucinaciones)
- **Factual Correctness**: Precisi√≥n factual vs ground truth (F1 score)

## üìö Implementaci√≥n ChunkRAG

### T√©cnicas Implementadas (4/7 del paper)

#### ‚úÖ 1. Semantic Chunking
**Archivo:** `src/chunking/semantic_chunk.py`

Basado en el paper (Secci√≥n 3):
- Tokenizaci√≥n por oraciones (NLTK)
- Embeddings: `text-embedding-3-small`
- Threshold de similitud: **Œ∏ = 0.8**
- L√≠mite de chunk: **128 tokens (~500 chars)**
- Agrupaci√≥n por cosine similarity

#### ‚úÖ 2. Multi-stage LLM Filtering (Async Optimizado)
**Archivo:** `src/filtering/chunk_filter.py`

Basado en el paper (Secci√≥n 3.2):

1. **Base Score**: LLM eval√∫a relevancia inicial (0-1)
2. **Self-Reflection**: LLM reflexiona y ajusta score
3. **Critic Evaluation**: Evaluaci√≥n cr√≠tica con heur√≠sticas
4. **Score Final**: `0.3 * base + 0.3 * reflect + 0.4 * critic`

**Optimizaci√≥n async:**
- Procesamiento paralelo de chunks con `asyncio.gather`
- **4x m√°s r√°pido** que versi√≥n secuencial
- Tiempo: ~10-15s vs ~45s

#### ‚úÖ 3. Dynamic Thresholding
**Archivo:** `src/filtering/chunk_filter.py:154-177`

```python
threshold = Œº + œÉ if var < Œµ else Œº
```

Adaptaci√≥n autom√°tica seg√∫n distribuci√≥n de scores.

#### ‚úÖ 4. Chunk-level Filtering
**Archivo:** `src/filtering/chunk_filter.py`

Filtrado granular a nivel de chunk (no documento completo).

### T√©cnicas No Implementadas

- ‚ùå **Redundancy removal** (similitud >0.9)
- ‚ùå **Hybrid retrieval** (BM25 + embeddings)
- ‚ùå **Cohere reranking** (anti "Lost in the middle")

*Nota: El sistema actual funciona bien sin estas optimizaciones adicionales.*

## ‚ö° Performance

### Velocidad de Filtering

| Versi√≥n | Tiempo (15 chunks) | Optimizaci√≥n |
|---------|-------------------|--------------|
| Secuencial | ~45s | Baseline |
| **Async Paralelo** | **~11s** | **4x m√°s r√°pido** |

### M√©tricas de Calidad (RAGAS)

**Pipeline de producci√≥n (semantic + filtering):**
- Context Recall: 0.70
- Faithfulness: 0.99
- Factual Correctness: 0.35

## ü§ñ DSPy: Optimizaci√≥n Autom√°tica de Prompts

**DSPy** (Declarative Self-improving Language Programs) es un framework de Stanford que permite **optimizar prompts autom√°ticamente** en lugar de escribirlos manualmente.

### ¬øPor qu√© DSPy?

El sistema usa **3 prompts diferentes** para filtering multi-etapa (Base, Reflection, Critic). Tradicionalmente:
- ‚ùå Escribir prompts manualmente
- ‚ùå Ajustar por prueba y error
- ‚ùå Dif√≠cil de mejorar sistem√°ticamente

Con DSPy:
- ‚úÖ Prompts optimizables autom√°ticamente
- ‚úÖ Entrenamiento con ejemplos
- ‚úÖ Mejora continua agregando datos

### Diferencia Clave: AsyncIO vs Threads

| Aspecto | Pipeline Original | Pipeline DSPy |
|---------|------------------|---------------|
| Procesamiento | AsyncIO | ThreadPoolExecutor |
| Velocidad | ~2-3s | ~3-4s |
| Prompts | Hardcoded | Optimizables |
| Paralelismo | `asyncio.gather` | `ThreadPoolExecutor(max_workers=15)` |

**¬øPor qu√© threads en DSPy?**

DSPy usa llamadas s√≠ncronas a OpenAI internamente, por lo que `asyncio` no funciona. La soluci√≥n es `ThreadPoolExecutor`:

```python
# Original (AsyncIO)
async def score_chunk(chunk, query):
    response = await llm.ainvoke(prompt)  # Async nativo
    return score

results = await asyncio.gather(*tasks)  # Paralelo

# DSPy (Threads)
def score_chunk_dspy(scorer, chunk, query):
    scores = scorer(chunk=chunk, query=query)  # Sync
    return scores

with ThreadPoolExecutor(max_workers=15) as executor:
    futures = [executor.submit(score_chunk_dspy, ...) for chunk in chunks]
    results = [f.result() for f in as_completed(futures)]  # Paralelo
```

Ambos m√©todos logran **paralelismo real**, pero con diferentes mecanismos internos.

### Entrenamiento DSPy

```bash
# Entrenar con 8 ejemplos (high/medium/low relevance)
uv run python scripts/train_dspy.py

# Salida:
# - Bootstrapped 4 full traces
# - 2 rondas de optimizaci√≥n
# - Genera: pipelines/production/compiled_scorer.json
```

El archivo `compiled_scorer.json` (10 KB) contiene:
- Prompts optimizados
- Ejemplos few-shot seleccionados
- Configuraci√≥n del scorer

### Comparaci√≥n de Resultados

**Mismo query:** "¬øCu√°l es la arquitectura de Belcorp?"

| M√©trica | Pipeline Original | DSPy Optimizado |
|---------|------------------|-----------------|
| Tiempo | 2s | 3s |
| Chunks filtrados | 6 | 7 |
| Threshold | 0.389 | 0.307 |

**Chunks m√°s relevantes (ambos incluyen):**
- Propuesta T√©cnica Plataforma de Anal√≠tica
- Reutilizar activos tecnol√≥gicos
- Componentes modulares

**Observaci√≥n:** DSPy selecciona chunks ligeramente diferentes pero relevantes. Requiere evaluaci√≥n con RAGAS para validar calidad.

### Pr√≥ximos Pasos con DSPy

1. **Evaluar con RAGAS**: Comparar m√©tricas vs pipeline original
2. **Ampliar dataset**: Agregar m√°s ejemplos (50-100)
3. **Experimentar con 2 etapas**: Quiz√°s Base + Critic sea suficiente
4. **Optimizadores avanzados**: MIPROv2, COPRO

üìö **Documentaci√≥n completa:** [`docs/DSPY_IMPLEMENTATION.md`](docs/DSPY_IMPLEMENTATION.md)

## üîß Configuraci√≥n Avanzada

### Desactivar Async Filtering

```python
from src.filtering import filter_chunks_by_relevance

# Usar versi√≥n secuencial
chunks = filter_chunks_by_relevance(
    chunks,
    query,
    use_async=False  # Secuencial (legacy)
)
```

### Ajustar Par√°metros de Producci√≥n

Editar `pipelines/production/config.py`:

```python
ENABLE_FILTERING = True         # Activar/desactivar filtering
INITIAL_RETRIEVAL_K = 15        # Candidatos iniciales
FINAL_CHUNKS_K = 5              # Chunks finales
SEMANTIC_THRESHOLD = 0.8        # Umbral de similitud
```

## üîó Referencias

- Paper: [ChunkRAG (arXiv:2410.19572v5)](https://arxiv.org/abs/2410.19572)
- Chonkie: [https://docs.chonkie.ai](https://docs.chonkie.ai)
- RAGAS: [https://docs.ragas.io](https://docs.ragas.io)
- OpenAI Embeddings: [text-embedding-3-large](https://platform.openai.com/docs/guides/embeddings)

## üìÑ Licencia

Proyecto educacional/investigaci√≥n.
